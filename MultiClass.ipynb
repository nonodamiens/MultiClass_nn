{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A MultiClass Neural Network #\n",
    "\n",
    "Let's try coding a Multiclass neural network from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import useful librairies\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " _ \n",
      "|_ \n",
      "|_|\n",
      "label :  [6]\n",
      " _ \n",
      " _|\n",
      " _|\n",
      "label :  [3]\n",
      " _ \n",
      "  |\n",
      "  |\n",
      "label :  [7]\n",
      "   \n",
      "|_|\n",
      "  |\n",
      "label :  [4]\n",
      " _ \n",
      "|_ \n",
      "|_|\n",
      "label :  [6]\n",
      " _ \n",
      "|_|\n",
      " _|\n",
      "label :  [9]\n",
      " _ \n",
      " _|\n",
      "|_ \n",
      "label :  [2]\n",
      " _ \n",
      "|_ \n",
      "|_|\n",
      "label :  [6]\n",
      " _ \n",
      "  |\n",
      "  |\n",
      "label :  [7]\n",
      "   \n",
      "|_|\n",
      "  |\n",
      "label :  [4]\n",
      " _ \n",
      " _|\n",
      " _|\n",
      "label :  [3]\n",
      " _ \n",
      "  |\n",
      "  |\n",
      "label :  [7]\n",
      " _ \n",
      "  |\n",
      "  |\n",
      "label :  [7]\n",
      " _ \n",
      " _|\n",
      "|_ \n",
      "label :  [2]\n",
      " _ \n",
      "|_ \n",
      " _|\n",
      "label :  [5]\n",
      "   \n",
      "|_|\n",
      "  |\n",
      "label :  [4]\n",
      "   \n",
      "  |\n",
      "  |\n",
      "label :  [1]\n",
      " _ \n",
      "  |\n",
      "  |\n",
      "label :  [7]\n",
      " _ \n",
      "|_ \n",
      " _|\n",
      "label :  [5]\n",
      "   \n",
      "  |\n",
      "  |\n",
      "label :  [1]\n"
     ]
    }
   ],
   "source": [
    "# Let's make a multilabel dataset\n",
    "\n",
    "# Initialize randomness\n",
    "np.random.seed(42)\n",
    "\n",
    "# Our dataset is made of coded numbers\n",
    "zero = np.array([[1],[1],[1],[0],[1],[1],[1]])\n",
    "one = np.array([[0],[0],[0],[0],[0],[1],[1]])\n",
    "two = np.array([[0],[1],[1],[1],[1],[1],[0]])\n",
    "three = np.array([[0],[0],[1],[1],[1],[1],[1]])\n",
    "four = np.array([[1],[0],[0],[1],[0],[1],[1]])\n",
    "five = np.array([[1],[0],[1],[1],[1],[0],[1]])\n",
    "six = np.array([[1],[1],[1],[1],[1],[0],[1]])\n",
    "seven = np.array([[0],[0],[1],[0],[0],[1],[1]])\n",
    "eight = np.array([[1],[1],[1],[1],[1],[1],[1]])\n",
    "nine = np.array([[1],[0],[1],[1],[1],[1],[1]])\n",
    "\n",
    "# A function to decode our number\n",
    "def decode_num(x):\n",
    "    r = x\n",
    "    r = np.insert(r, 0, [0])\n",
    "    r = np.insert(r, -2, [0])\n",
    "    r = r.reshape(9,1)\n",
    "    r = np.append(r[:3], np.append(r[3:6], r[6:], axis = 1), axis = 1)\n",
    "    stringa = ''\n",
    "    stringb = ''\n",
    "    stringc = ''\n",
    "    for i,y in enumerate(r):\n",
    "        for j,v in enumerate(y):\n",
    "            if i == 0:\n",
    "                if v == 1 and j == 1:\n",
    "                    stringa = stringa + '_'\n",
    "                else:\n",
    "                    stringa = stringa + ' '\n",
    "            if i == 1:\n",
    "                if v == 1 and j == 1:\n",
    "                    stringb = stringb + '_'\n",
    "                elif v == 1:\n",
    "                    stringb = stringb + '|'\n",
    "                else:\n",
    "                    stringb = stringb + ' '\n",
    "            if i == 2:\n",
    "                if v == 1 and j == 1:\n",
    "                    stringc = stringc + '_'\n",
    "                elif v == 1:\n",
    "                    stringc = stringc + '|'\n",
    "                else:\n",
    "                    stringc = stringc + ' '\n",
    "    print(stringa)\n",
    "    print(stringb)\n",
    "    print(stringc)\n",
    "\n",
    "# Making m examples from our coded numbers\n",
    "m = 20\n",
    "# Choosing randomly the numbers for m examples\n",
    "pre_dataset = np.random.randint(0, 10, (1, m))\n",
    "\n",
    "# Setting labels\n",
    "Y = np.zeros(0)\n",
    "for v in pre_dataset[0]:\n",
    "    Y = np.append(Y, [0 for x in range(v)] +[1] + [0 for x in range(9-v)])\n",
    "Y = Y.reshape(m,10)\n",
    "\n",
    "# Setting inputs\n",
    "numbers = [zero, one, two, three, four, five, six, seven, eight, nine]\n",
    "X = np.zeros(0)\n",
    "for v in pre_dataset[0]:\n",
    "    X = np.append(X, [numbers[v]])\n",
    "X = X.reshape(m,7)\n",
    "\n",
    "# Let's check if everything works find\n",
    "for i, v in enumerate(Y):\n",
    "    decode_num(X[i])\n",
    "    print('label : ' , np.where(v == 1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation function\n",
    "def activation(x, function = 'sigmoid'):\n",
    "    if function == 'sigmoid':\n",
    "        return 1/(1+np.exp(-x))\n",
    "    else:\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape :  (20, 7)\n",
      "Y shape :  (20, 10)\n",
      "Archi :  [7, 3, 10]\n",
      "LOOP :  0\n",
      "Loss :  5.487801580455904\n",
      "Accuracy :  89.59976162257247\n",
      "LOOP :  1\n",
      "Loss :  5.42928721093797\n",
      "Accuracy :  89.60674295635303\n",
      "LOOP :  2\n",
      "Loss :  5.377661363910407\n",
      "Accuracy :  89.61403585679248\n",
      "LOOP :  3\n",
      "Loss :  5.332276741925242\n",
      "Accuracy :  89.62166879176897\n",
      "LOOP :  4\n",
      "Loss :  5.292728892604638\n",
      "Accuracy :  89.6296652592581\n",
      "LOOP :  5\n",
      "Loss :  5.258810579251351\n",
      "Accuracy :  89.63804224846481\n",
      "LOOP :  6\n",
      "Loss :  5.230476111207694\n",
      "Accuracy :  89.64680623348708\n",
      "LOOP :  7\n",
      "Loss :  5.207807964267763\n",
      "Accuracy :  89.65594745854294\n",
      "LOOP :  8\n",
      "Loss :  5.1909811994937325\n",
      "Accuracy :  89.66543352952114\n",
      "LOOP :  9\n",
      "Loss :  5.180223560905816\n",
      "Accuracy :  89.67520367057728\n",
      "LOOP :  10\n",
      "Loss :  5.175771379119773\n",
      "Accuracy :  89.68516530879644\n",
      "LOOP :  11\n",
      "Loss :  5.177823781377303\n",
      "Accuracy :  89.69519467707777\n",
      "LOOP :  12\n",
      "Loss :  5.186500007456279\n",
      "Accuracy :  89.70514260676865\n",
      "LOOP :  13\n",
      "Loss :  5.201806179503447\n",
      "Accuracy :  89.71484549478615\n",
      "LOOP :  14\n",
      "Loss :  5.223617704635168\n",
      "Accuracy :  89.72413981418124\n",
      "LOOP :  15\n",
      "Loss :  5.25168094280006\n",
      "Accuracy :  89.7328771524842\n",
      "LOOP :  16\n",
      "Loss :  5.285633304690575\n",
      "Accuracy :  89.7409363867756\n",
      "LOOP :  17\n",
      "Loss :  5.325036345667906\n",
      "Accuracy :  89.74823056361129\n",
      "LOOP :  18\n",
      "Loss :  5.369413843413403\n",
      "Accuracy :  89.75470788023723\n",
      "LOOP :  19\n",
      "Loss :  5.4182872857894555\n",
      "Accuracy :  89.76034790937757\n",
      "LOOP :  20\n",
      "Loss :  5.471203887136691\n",
      "Accuracy :  89.76515513002535\n",
      "LOOP :  21\n",
      "Loss :  5.5277554905054895\n",
      "Accuracy :  89.76915180618789\n",
      "LOOP :  22\n",
      "Loss :  5.587589120591865\n",
      "Accuracy :  89.77237166687668\n",
      "LOOP :  23\n",
      "Loss :  5.6504110697770695\n",
      "Accuracy :  89.77485514238384\n",
      "LOOP :  24\n",
      "Loss :  5.715986480588682\n",
      "Accuracy :  89.77664638103383\n",
      "LOOP :  25\n",
      "Loss :  5.784135914419568\n",
      "Accuracy :  89.77779197067521\n",
      "LOOP :  26\n",
      "Loss :  5.854729762983041\n",
      "Accuracy :  89.77834115836852\n",
      "LOOP :  27\n",
      "Loss :  5.927680794386605\n",
      "Accuracy :  89.77834730004099\n",
      "LOOP :  28\n",
      "Loss :  6.002934744518178\n",
      "Accuracy :  89.77787018742934\n",
      "LOOP :  29\n",
      "Loss :  6.0804587407293065\n",
      "Accuracy :  89.7769787291668\n",
      "LOOP :  30\n",
      "Loss :  6.160227551024615\n",
      "Accuracy :  89.7757531943852\n",
      "LOOP :  31\n",
      "Loss :  6.2422082420389104\n",
      "Accuracy :  89.77428593874033\n",
      "LOOP :  32\n",
      "Loss :  6.326344763626905\n",
      "Accuracy :  89.7726794271909\n",
      "LOOP :  33\n",
      "Loss :  6.412545018140773\n",
      "Accuracy :  89.77104073488064\n",
      "LOOP :  34\n",
      "Loss :  6.500673614598999\n",
      "Accuracy :  89.76947273298073\n",
      "LOOP :  35\n",
      "Loss :  6.590553077733746\n",
      "Accuracy :  89.76806363200018\n",
      "LOOP :  36\n",
      "Loss :  6.6819743187678\n",
      "Accuracy :  89.76687771802403\n",
      "LOOP :  37\n",
      "Loss :  6.774714056785032\n",
      "Accuracy :  89.7659501101905\n",
      "LOOP :  38\n",
      "Loss :  6.868554131671675\n",
      "Accuracy :  89.76528696274208\n",
      "LOOP :  39\n",
      "Loss :  6.9632971124971705\n",
      "Accuracy :  89.76487049524509\n",
      "LOOP :  40\n",
      "Loss :  7.058774763366287\n",
      "Accuracy :  89.76466674002636\n",
      "LOOP :  41\n",
      "Loss :  7.1548492755006\n",
      "Accuracy :  89.76463360082542\n",
      "LOOP :  42\n",
      "Loss :  7.2514096037149045\n",
      "Accuracy :  89.76472750430588\n",
      "LOOP :  43\n",
      "Loss :  7.34836582532497\n",
      "Accuracy :  89.76490793239749\n",
      "LOOP :  44\n",
      "Loss :  7.445643672949515\n",
      "Accuracy :  89.76513991382711\n",
      "LOOP :  45\n",
      "Loss :  7.543180262069546\n",
      "Accuracy :  89.7653949491824\n",
      "LOOP :  46\n",
      "Loss :  7.640921180137393\n",
      "Accuracy :  89.76565092144294\n",
      "LOOP :  47\n",
      "Loss :  7.738818679540325\n",
      "Accuracy :  89.76589145367988\n",
      "LOOP :  48\n",
      "Loss :  7.836830609916987\n",
      "Accuracy :  89.76610503617711\n",
      "LOOP :  49\n",
      "Loss :  7.934919776794115\n",
      "Accuracy :  89.7662841188956\n",
      "LOOP :  50\n",
      "Loss :  8.033053511343905\n",
      "Accuracy :  89.7664242726645\n",
      "LOOP :  51\n",
      "Loss :  8.131203325506807\n",
      "Accuracy :  89.76652346314529\n",
      "LOOP :  52\n",
      "Loss :  8.229344590541416\n",
      "Accuracy :  89.76658144752487\n",
      "LOOP :  53\n",
      "Loss :  8.327456215872996\n",
      "Accuracy :  89.7665992865418\n",
      "LOOP :  54\n",
      "Loss :  8.425520325384136\n",
      "Accuracy :  89.76657895716417\n",
      "LOOP :  55\n",
      "Loss :  8.523521936796614\n",
      "Accuracy :  89.76652304947574\n",
      "LOOP :  56\n",
      "Loss :  8.621448651746812\n",
      "Accuracy :  89.76643453227643\n",
      "LOOP :  57\n",
      "Loss :  8.719290363007806\n",
      "Accuracy :  89.76631657394675\n",
      "LOOP :  58\n",
      "Loss :  8.817038983096305\n",
      "Accuracy :  89.76617240740258\n",
      "LOOP :  59\n",
      "Loss :  8.914688196286729\n",
      "Accuracy :  89.76600523007676\n",
      "LOOP :  60\n",
      "Loss :  9.012233234296003\n",
      "Accuracy :  89.76581813165178\n",
      "LOOP :  61\n",
      "Loss :  9.109670674716064\n",
      "Accuracy :  89.76561404371614\n",
      "LOOP :  62\n",
      "Loss :  9.206998260599288\n",
      "Accuracy :  89.76539570666213\n",
      "LOOP :  63\n",
      "Loss :  9.304214739321699\n",
      "Accuracy :  89.76516565004292\n",
      "LOOP :  64\n",
      "Loss :  9.40131971882732\n",
      "Accuracy :  89.76492618331852\n",
      "LOOP :  65\n",
      "Loss :  9.498313539482865\n",
      "Accuracy :  89.7646793944906\n",
      "LOOP :  66\n",
      "Loss :  9.595197159961907\n",
      "Accuracy :  89.764427154591\n",
      "LOOP :  67\n",
      "Loss :  9.69197205577889\n",
      "Accuracy :  89.76417112637391\n",
      "LOOP :  68\n",
      "Loss :  9.788640129275624\n",
      "Accuracy :  89.76391277588436\n",
      "LOOP :  69\n",
      "Loss :  9.88520363001405\n",
      "Accuracy :  89.76365338584758\n",
      "LOOP :  70\n",
      "Loss :  9.981665084647238\n",
      "Accuracy :  89.7633940700524\n",
      "LOOP :  71\n",
      "Loss :  10.078027235429879\n",
      "Accuracy :  89.7631357880948\n",
      "LOOP :  72\n",
      "Loss :  10.17429298659687\n",
      "Accuracy :  89.76287936000624\n",
      "LOOP :  73\n",
      "Loss :  10.270465357890535\n",
      "Accuracy :  89.76262548042244\n",
      "LOOP :  74\n",
      "Loss :  10.366547444559753\n",
      "Accuracy :  89.762374732055\n",
      "LOOP :  75\n",
      "Loss :  10.462542383192059\n",
      "Accuracy :  89.76212759831077\n",
      "LOOP :  76\n",
      "Loss :  10.558453322775923\n",
      "Accuracy :  89.7618844749712\n",
      "LOOP :  77\n",
      "Loss :  10.654283400426502\n",
      "Accuracy :  89.76164568089249\n",
      "LOOP :  78\n",
      "Loss :  10.750035721245103\n",
      "Accuracy :  89.76141146772595\n",
      "LOOP :  79\n",
      "Loss :  10.845713341820552\n",
      "Accuracy :  89.76118202868346\n",
      "LOOP :  80\n",
      "Loss :  10.941319256919275\n",
      "Accuracy :  89.76095750639249\n",
      "LOOP :  81\n",
      "Loss :  11.036856388949545\n",
      "Accuracy :  89.7607379998961\n",
      "LOOP :  82\n",
      "Loss :  11.132327579823798\n",
      "Accuracy :  89.76052357086095\n",
      "LOOP :  83\n",
      "Loss :  11.227735584880039\n",
      "Accuracy :  89.760314249059\n",
      "LOOP :  84\n",
      "Loss :  11.323083068559116\n",
      "Accuracy :  89.7601100371889\n",
      "LOOP :  85\n",
      "Loss :  11.418372601568453\n",
      "Accuracy :  89.75991091510184\n",
      "LOOP :  86\n",
      "Loss :  11.513606659294263\n",
      "Accuracy :  89.75971684349305\n",
      "LOOP :  87\n",
      "Loss :  11.608787621253512\n",
      "Accuracy :  89.75952776711696\n",
      "LOOP :  88\n",
      "Loss :  11.703917771403436\n",
      "Accuracy :  89.75934361757918\n",
      "LOOP :  89\n",
      "Loss :  11.798999299150632\n",
      "Accuracy :  89.75916431575448\n",
      "LOOP :  90\n",
      "Loss :  11.894034300923352\n",
      "Accuracy :  89.75898977387499\n",
      "LOOP :  91\n",
      "Loss :  11.98902478219012\n",
      "Accuracy :  89.75881989732876\n",
      "LOOP :  92\n",
      "Loss :  12.083972659824829\n",
      "Accuracy :  89.75865458620456\n",
      "LOOP :  93\n",
      "Loss :  12.178879764733665\n",
      "Accuracy :  89.75849373661478\n",
      "LOOP :  94\n",
      "Loss :  12.273747844672505\n",
      "Accuracy :  89.7583372418251\n",
      "LOOP :  95\n",
      "Loss :  12.368578567194856\n",
      "Accuracy :  89.75818499321547\n",
      "LOOP :  96\n",
      "Loss :  12.46337352268062\n",
      "Accuracy :  89.75803688109508\n",
      "LOOP :  97\n",
      "Loss :  12.558134227404372\n",
      "Accuracy :  89.75789279539052\n",
      "LOOP :  98\n",
      "Loss :  12.652862126609595\n",
      "Accuracy :  89.75775262622385\n",
      "LOOP :  99\n",
      "Loss :  12.747558597561346\n",
      "Accuracy :  89.75761626439602\n",
      "89.75761626439602\n"
     ]
    }
   ],
   "source": [
    "# The neural network\n",
    "\n",
    "# Initialisation\n",
    "# the architecture (hidden layers)\n",
    "N = [3]\n",
    "m = X.shape[0]\n",
    "\n",
    "# Adding the input and output layers\n",
    "N = [X.shape[1]] + N + [Y.shape[1]]\n",
    "\n",
    "# Check X, Y and N\n",
    "print('X shape : ', X.shape)\n",
    "print('Y shape : ', Y.shape)\n",
    "print('Archi : ', N)\n",
    "\n",
    "# Weights and bias\n",
    "np.random.seed(42)\n",
    "W = [np.random.randn(N[r], N[r+1]) for r in range(len(N)-1) ]\n",
    "b = [np.zeros((1, N[r+1])) for r in range(len(N)-1)]\n",
    "\n",
    "# Hyperparameters\n",
    "lr = 0.5\n",
    "epoch = 100\n",
    "costs = []\n",
    "\n",
    "# The training\n",
    "for i in range(epoch):\n",
    "    print('LOOP : ', i)\n",
    "    A = [X]\n",
    "    # Forward\n",
    "    for l in range(len(N)-1):\n",
    "#         print('LAYER : ', l+1)\n",
    "        # the last layer\n",
    "        if l == len(N) - 2:\n",
    "            # compute the softmax\n",
    "            Z = np.dot(A[l], W[l]) + b[l] \n",
    "            Ypred = np.exp(Z) / np.sum(np.exp(Z))\n",
    "        else:\n",
    "#             print(A[l].shape)\n",
    "#             print(W[l].shape)\n",
    "            Z = np.dot(A[l], W[l]) + b[l]\n",
    "            A.append(activation(Z))\n",
    "#         print(A[l].shape)\n",
    "    \n",
    "#     print('Ypred : ', Ypred.shape)\n",
    "    # Cost\n",
    "    L = - np.sum(Y * np.log(Ypred)) / m\n",
    "    print('Loss : ', L)\n",
    "    costs.append(L)\n",
    "    accuracy  = (1 - np.mean(np.abs(Ypred - Y))) * 100\n",
    "    print('Accuracy : ', accuracy)\n",
    "    \n",
    "    # Backward\n",
    "    dw = []\n",
    "    db = []\n",
    "    dz = Ypred - Y # (m 10)\n",
    "    dw.insert(0, np.dot(A[-1].T, dz) / m) # (m p)\n",
    "    db.insert(0, np.sum(dz, axis = 0, keepdims=True) / m) # (1 p)\n",
    "    for l in reversed(range(len(N) - 2)):\n",
    "        dz = np.dot(dz, W[l+1].T) * (A[l+1] * (1 - A[l+1]))\n",
    "        dw.insert(0, np.dot(A[l].T, dz) / m)\n",
    "        db.insert(0, np.sum(dz, axis = 0, keepdims=True) / m)\n",
    "    \n",
    "#     for v in dw:\n",
    "#         print('dw : ', v.shape)\n",
    "    \n",
    "    # Update\n",
    "    for l in range(len(N)-1):\n",
    "#         print('update : ',W[l].shape)\n",
    "#         print(dw[l].shape)\n",
    "        W[l] = W[l] - lr * dw[l]\n",
    "        b[l] = b[l] - lr * db[l]\n",
    "        \n",
    "accuracy  = (1 - np.mean(np.abs(Ypred - Y))) * 100\n",
    "print(accuracy)\n",
    "    \n",
    "        \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
